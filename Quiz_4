### Question 1

library(ElemStatLearn)
library(caret) 
library(forecast)
data(vowel.train)
data(vowel.test) 

train.df <- as.data.frame(vowel.train)
test.df <- as.data.frame(vowel.test)
train.df$y <- as.factor(train.df$y)
test.df$y <- as.factor(test.df$y)

set.seed(33833)

model_rf <- train( y ~ . , data = train.df , method="rf")
model_gbm <- train( y ~ . , data = train.df , method="gbm")



model_rf
model_gbm

predict_rf <- as.data.frame(predict(model_rf , newdata = test.df))
predict_gbm <- as.data.frame(predict(model_gbm , newdata = test.df))

confusionMatrix(predict_rf[,1] , test.df$y)$overall[1]
confusionMatrix(predict_gbm[,1] , test.df$y)$overall[1]

concurrance <- predict_rf[,1]==predict_gbm[,1]

test_concur <- test.df[concurrance,]
test_conc.df <- predict(model_rf , newdata = test_concur)
confusionMatrix(test_conc.df , test_concur$y)$overall[1]


concur_frac <- length(concurrance[which(concurrance==TRUE)])/length(concurrance)
concur_frac

#Set the variable y to be a factor variable in both the training and test set. Then set the seed to 33833. Fit (1) a random forest predictor relating the factor variable y to the remaining variables and (2) a boosted predictor using the "gbm" method. Fit these both with the train() command in the caret package. 

#What are the accuracies for the two approaches on the test data set? What is the accuracy among the test set samples where the two methods agree?

###  Select this
#RF Accuracy = 0.6082 
#GBM Accuracy = 0.5152 
#Agreement Accuracy = 0.6361

#RF Accuracy = 0.6082 
#GBM Accuracy = 0.5152 
#Agreement Accuracy = 0.5152

#RF Accuracy = 0.6082 
#GBM Accuracy = 0.5152 
#Agreement Accuracy = 0.5325

#RF Accuracy = 0.9881 
#GBM Accuracy = 0.8371 
#Agreement Accuracy = 0.9983


### Question 2

library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

set.seed(62433)

model_rf <- train( diagnosis ~ . , data = training , method="rf")
model_gbm <- train( diagnosis  ~ . , data = training , method="gbm")
model_lda <- train( diagnosis  ~ . , data = training , method="lda")

predict_rf <- predict(model_rf , newdata = testing)
predict_gbm <- predict(model_gbm , newdata = testing)
predict_lda <- predict(model_lda , newdata = testing)

confusionMatrix(predict_rf , testing$diagnosis)$overall[1]
confusionMatrix(predict_gbm , testing$diagnosis)$overall[1]
confusionMatrix(predict_lda , testing$diagnosis)$overall[1]

pred.df <- data.frame(predict_rf , predict_gbm , predict_lda , diagnosis = testing$diagnosis)
combModFit <- train(diagnosis~. , data=pred.df ,  method="rf")
combPred <- predict(combModFit,pred.df)
confusionMatrix(combPred,testing$diagnosis)$overall[1]


# Set the seed to 62433 and predict diagnosis with all the other variables using a random forest ("rf"),
# boosted trees ("gbm") and linear discriminant analysis ("lda") model. 
# Stack the predictions together using random forests ("rf"). What is the resulting accuracy on the test set? 
# Is it better or worse than each of the individual predictions?

# Stacked Accuracy: 0.76 is better than lda but not random forests or boosting.

### Select this (below) as closest
# Stacked Accuracy: 0.80 is better than all three other methods
# Stacked Accuracy: 0.88 is better than all three other methods
# Stacked Accuracy: 0.80 is better than random forests and lda and the same as boosting.

### Question 3

# Load the concrete data with the commands:

set.seed(3523)
library(ElemStatLearn)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]

set.seed(233) 

model <- train(CompressiveStrength ~ . , data = training , model='lasso')
model$finalModel
plot.enet(model$finalModel, xvar = c("fraction", "penalty", "L1norm", "step"))
plot(model$finalModel, xvar = c( "penalty"))
no_idea <- predict.enet(model$finalModel, type='coefficients', s=model$bestTune$fraction, mode='fraction')

plot.enet(no_idea$finalModel)

plot.enet(model$finalModel, "penalty")
plot.enet(model$finalModel, xvar="penalty", use.color=TRUE)
# fit a lasso model to predict Compressive Strength. 
# Which variable is the last coefficient to be set to zero as the penalty increases?
# (Hint: it may be useful to look up ?plot.enet).

# Answers:
# Water
# FineAggregate
# Answer below == cement
# Cement
# BlastFurnaceSlag

### Question 4

data <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv")
library(forecast)  
library(lubridate)  # For year() function below
dat = read.csv(textConnection(data))
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)  #time series 

# Fit a model using the bats() function in the forecast package to the training time series. 
# Then forecast this model for the remaining time points. For how many of the testing points 
# is the true value within the 95% prediction interval bounds?

plot(tstrain , xlab = 'Days' , ylab = 'Hits')
plot(decompose(tstrain) , ylab = 'Hits')

fit <- bats(tstrain) 

# h = 235 periods which is the number of test points
# level = 95 at the 95% confidence interval

forecasted_hits <- as.data.frame(forecast(fit , h = 235 , level = 95))
plot(forecasted_hits)

inRange = 0

for (i in 1:nrow(testing)){
     print(c(i ,forecasted_hits[i,2] , testing[i,3] , forecasted_hits[i,3]))
  
     if(testing[i,3]<forecasted_hits[i,3] && testing[i,3]>forecasted_hits[i,2]){
       inRange = inRange + 1
         }
    }
inRange
percent_inRange <- inRange / nrow(testing)
percent_inRange

#Answer == 96%
# 96%
# 93%
# 98%
# 100%

### Question 5

#Load the concrete data with the commands:

set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)

model <- svm(CompressiveStrength ~ ., data = training)
pred.train <- predict(model, training) #  prediction on training data
pred.test <- predict(model, testing) #  prediction on testing data
results <- data.frame(pred.test , testing$CompressiveStrength)

n = nrow(results)
print(c('RMS Error of the predicted and Observed Strength = ' , sqrt(sum((results[,1]-results[,2])^2)/n)))
## Answer = 6.72

# fit a support vector machine using the e1071 package to predict Compressive Strength 
# using the default settings. Predict on the testing set. What is the RMSE?

# 6.72
# 11543.39
# 107.44
# 6.93
