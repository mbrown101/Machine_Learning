### Machine Learning
### Q1 create training and test sets with about 50% of observations assigned to each

library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
summary(diagnosis) # note there is a DF and a factor vector
adData = data.frame(diagnosis,predictors) # create single DF with all data
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE) # create DF 1 col, len*p rows
training = adData[trainIndex,] # select rows in trainIndex
testing = adData[-trainIndex,] # select rows not in trainIndex

### Q2  Make a histogram and confirm the SuperPlasticizer variable is skewed. 
# Normally you might use the log transform to try to make the data more symmetric. 
# Why would that be a poor choice for this variable?

library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(testing$Superplasticizer)
hist(log10(testing$Superplasticizer)) #log base 10 of variables
summary(log10(testing$Superplasticizer))  # results in Inf numbers
summary(testing$Superplasticizer) # there are values == 0.  no negative values
### answer:  There are values of zero so when you take the log() transform those values will be -Inf.


### Q3 Find all the predictor variables in the training set that begin with IL. 
# Perform principal components on these variables with the preProcess() function 
# from the caret package. 
# Calculate the number of principal components needed to capture 80% of the variance.
# How many are there?

library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
colnames_2char <-  substr(names(training) , 1 , 2) 
colnames_IL <- which( colnames_2char  == 'IL' )
train_IL <- training[  , c( colnames_IL )  ] 

preProcess(train_IL , thresh = .80 , method= 'pca')



### Q4 Create a training data set consisting of only the predictors with 
# variable names beginning with IL and the diagnosis. 
# Build two predictive models, one using the predictors as they are and one using 
# PCA with principal components explaining 80% of the variance in the predictors. 
# Use method="glm" in the train function. 
# What is the accuracy of each method in the test set? 
# Which is more accurate?
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
colnames_2char <-  substr(names(training) , 1 , 2) 
colnames_IL <- which( colnames_2char  == 'IL' )
train_IL_diag <- training[  , c( colnames_IL , 1 )  ] # add col 1 for diagnosis and IL cols
all_pred_mod <- train(diagnosis ~ . , data = train_IL_diag , method = 'glm') 
all_pred_mod

colnames_2char <-  substr(names(training) , 1 , 2) 
colnames_IL <- which( colnames_2char  == 'IL' )
train_IL <- training[  , c( colnames_IL)  ] 

reduced_var <- preProcess(train_IL , thresh = .80 , method= 'pca')
reduced_var_data <- predict(reduced_var , training[ , c( colnames_IL ) ] )
reduced_pred_mod <- train(diagnosis ~ . , data = training , method = 'glm') 
reduced_pred_mod

