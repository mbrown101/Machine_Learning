---
title: "Analysis of Personal Fitness Data"
author: "Michael Brown"
date: "September 2015"
output: html_document
---
### Executive Summary
This study predicts fitness outcomes for 20 different test cases using the boost gbm algorithm and machine learning techniques executed in an R environment. This study successfully predicted  outcomes in 20 out of 20 tests on the first run. Based on cross validation, the method was estimated to be 96.53% accurate. 

### Description of Experiment 

Experiment: Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. This purpose of this experiment is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and determine if they are performing perform barbell lifts correctly or incorrectly.  

Data: Data for this experiment is sourced from the Human Activity Recognition (HAR) project at http://groupware.les.inf.puc-rio.br/har.   The data was generated by six young health participants who were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl while wearing a set of accelerometers.  The  exercise was performed in one of five ways: 
Class A - exactly according to the specification 
Class B - throwing the elbows to the front 
Class C - lifting the dumbbell only halfway 
Class D - lowering the dumbbell only halfway 
Class E - throwing the hips to the front 


```{r}

library(caret)
library(rattle)
library(splines)
library(parallel)
library(plyr)
require(RCurl)

train.csv <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
df.train.data <- read.csv(textConnection(train.csv))

test.csv <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
df.test.data <- as.data.frame(read.csv(textConnection(test.csv)))

```


### Data Exploration
```{r, echo=FALSE}
The data from the HAR project consists of a training set and a test set.  The training set is made up of 160 variables and 19622 observations while the testing set consists of 160 variables and 20 observations or prediction instances.  The data presents no intuitive pathway to inform an initial model thus we look at the quality of the data and first focus on conditioning it for analysis. 

```


### Data Conditioning
```{r, echo=FALSE}

Although the data suggests no intuitave model, the format of the data and the data types themselves suggest several areas for pre-processing:

- NAs: looking first at the testing set, all columns with NAs were omitted as they bring no additional value to the analysis and potentially cause the model to fail if it is build on data that doesnt exist in the testing set.  This had the effect of reducing the test data from 160 variables to 53 variables.    

- Match training and test data: Once the NAs were removed from the testing set, the same columns were removed from the training set in order to avoid modeling on data that was not availible for predicting using the test set.  This has the added benefit of reducing the amount of data processed during the computationally intensive training process.

- Remove non-relatent data: The training data consisted of a variety of metadata including the person conducting the exercise, time stamps and rwo identifiers. These data were removed as they had no effect on the outcome of the exercise and would otherwise have further complicated the training step


names.test.not_na <- !is.na(df.test.data[1,])  # list of data columns to keep without NAs
df.test.not_na <- df.test.data[,names.test.not_na]  # Reformed test df with just columns with data

names.df.test <- names(df.test.not_na[1,which(df.test.not_na[1,]!='')]) # list of data columns not empty
df.test.meta <- df.test.not_na[,names.df.test] # reformed data with no empty columns

df.test <- df.test.meta[,8:60]  # remove metadata

# Remove columns from training that are not found in pre-processed test data (except classe)

train.names.keep <- is.element(names(df.train.data) , names(df.test))

df.train.match <- df.train.data[,train.names.keep]
df.train.all <- cbind(df.train.match , df.train.data[,'classe'])

colnames(df.train.all)[53] <- 'classe'

in.train <- createDataPartition(y = df.train.all$classe , p = .85 , list = FALSE)

df.train <- df.train.all[in.train , ]
df.val <- df.train.all[-in.train , ]

```


### Model
```{r, echo=FALSE}
    
### Solution by Boost-GBM

#  Create model
control <- trainControl(method="cv", number=10)
model_gbm <- train(classe~. , data = df.train , method="gbm" , trControl=control ,  verbose = FALSE )
model_gbm$finalModel
model_gbm

#  Visualization
plot(varImp(model_gbm))

# Cross Validation
cross_validation_results_boost <- predict(model_gbm , newdata = df.val)
confusionMatrix(df.val$classe , cross_validation_results_boost)

#  Prediction 
predict.test.gbm <- predict(model_gbm , newdata = df.test )
predict.test.gbm

#produce result files
n = length(predict.test.gbm)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(predict.test.gbm[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }

```

### Results
```{r, echo=FALSE}
The resulting data produced from the boost modeling of the HAR data presented in order from 1 to 20 are:
  
predict.test.gbm  

```
